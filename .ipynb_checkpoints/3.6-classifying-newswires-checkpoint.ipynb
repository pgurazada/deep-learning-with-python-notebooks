{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
    "But what happens when you have more than two classes? \n",
    "\n",
    "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
    "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 9s 4us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 2s 4us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 214us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.8696 - acc: 0.8165 - val_loss: 1.0802 - val_acc: 0.7580\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.7032 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7820\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 0.5664 - acc: 0.8801 - val_loss: 0.9418 - val_acc: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.4582 - acc: 0.9047 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 0.3696 - acc: 0.9228 - val_loss: 0.9369 - val_acc: 0.7910\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 173us/step - loss: 0.3033 - acc: 0.9313 - val_loss: 0.8918 - val_acc: 0.8080\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.2539 - acc: 0.9416 - val_loss: 0.9069 - val_acc: 0.8100\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 0.2189 - acc: 0.9465 - val_loss: 0.9201 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 174us/step - loss: 0.1878 - acc: 0.9508 - val_loss: 0.9054 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 173us/step - loss: 0.1706 - acc: 0.9528 - val_loss: 0.9342 - val_acc: 0.8100\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.1537 - acc: 0.9551 - val_loss: 0.9664 - val_acc: 0.8070\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 178us/step - loss: 0.1393 - acc: 0.9557 - val_loss: 0.9687 - val_acc: 0.8140\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 0.1317 - acc: 0.9562 - val_loss: 1.0263 - val_acc: 0.8040\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 0.1219 - acc: 0.9579 - val_loss: 1.0322 - val_acc: 0.7960\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.1202 - acc: 0.9579 - val_loss: 1.0445 - val_acc: 0.8070\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.1139 - acc: 0.9595 - val_loss: 1.1004 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.1117 - acc: 0.9598 - val_loss: 1.0704 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('gray')\n",
    "sns.set_style('ticks', {'grid.color' : '0.9'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAElCAYAAADHpsRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwSKEEEKtJFiEEEKo1f8Dbl/EthB6oF0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26096413eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'o', label='Training loss', markersize = 4)\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAElCAYAAADHpsRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260d2b4fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'o', label='Training acc', markersize=4)\n",
    "plt.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 214us/step - loss: 2.5398 - acc: 0.5226 - val_loss: 1.6733 - val_acc: 0.6570\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 1.3712 - acc: 0.7121 - val_loss: 1.2758 - val_acc: 0.7210\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 1.0136 - acc: 0.7781 - val_loss: 1.1303 - val_acc: 0.7530\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 0.7976 - acc: 0.8251 - val_loss: 1.0539 - val_acc: 0.7590\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.6393 - acc: 0.8624 - val_loss: 0.9754 - val_acc: 0.7920\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 172us/step - loss: 0.5124 - acc: 0.8923 - val_loss: 0.9102 - val_acc: 0.8140\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.4123 - acc: 0.9137 - val_loss: 0.8932 - val_acc: 0.8210\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.3354 - acc: 0.9288 - val_loss: 0.8732 - val_acc: 0.8260\n",
      "2246/2246 [==============================] - 0s 193us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9847470230007427, 0.7845057880676759]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19679430097951914"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 297us/step - loss: 2.6911 - acc: 0.3727 - val_loss: 2.0459 - val_acc: 0.4000\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 249us/step - loss: 1.7529 - acc: 0.5053 - val_loss: 1.5791 - val_acc: 0.6140\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 242us/step - loss: 1.3783 - acc: 0.6438 - val_loss: 1.4144 - val_acc: 0.6320\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 245us/step - loss: 1.1762 - acc: 0.6877 - val_loss: 1.3323 - val_acc: 0.6710\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 246us/step - loss: 1.0398 - acc: 0.7398 - val_loss: 1.2792 - val_acc: 0.7080\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 246us/step - loss: 0.9361 - acc: 0.7680 - val_loss: 1.2896 - val_acc: 0.7070\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 245us/step - loss: 0.8572 - acc: 0.7831 - val_loss: 1.2841 - val_acc: 0.7120\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 252us/step - loss: 0.7925 - acc: 0.7963 - val_loss: 1.2997 - val_acc: 0.7170\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 245us/step - loss: 0.7370 - acc: 0.8104 - val_loss: 1.3377 - val_acc: 0.7120\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 247us/step - loss: 0.6898 - acc: 0.8202 - val_loss: 1.3560 - val_acc: 0.7260\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 247us/step - loss: 0.6503 - acc: 0.8237 - val_loss: 1.3784 - val_acc: 0.7200\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 247us/step - loss: 0.6155 - acc: 0.8292 - val_loss: 1.3911 - val_acc: 0.7180\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 262us/step - loss: 0.5843 - acc: 0.8344 - val_loss: 1.4235 - val_acc: 0.7220\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 263us/step - loss: 0.5540 - acc: 0.8404 - val_loss: 1.4961 - val_acc: 0.7210\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 251us/step - loss: 0.5303 - acc: 0.8436 - val_loss: 1.4768 - val_acc: 0.7270\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 233us/step - loss: 0.5082 - acc: 0.8479 - val_loss: 1.5435 - val_acc: 0.7200\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 235us/step - loss: 0.4870 - acc: 0.8544 - val_loss: 1.5616 - val_acc: 0.7210\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 236us/step - loss: 0.4673 - acc: 0.8652 - val_loss: 1.5850 - val_acc: 0.7180\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 260us/step - loss: 0.4480 - acc: 0.8712 - val_loss: 1.6502 - val_acc: 0.7140\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 254us/step - loss: 0.4320 - acc: 0.8763 - val_loss: 1.6857 - val_acc: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x260d4d9b828>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 3.0177 - acc: 0.5054 - val_loss: 2.2360 - val_acc: 0.6020\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 147us/step - loss: 1.8710 - acc: 0.6501 - val_loss: 1.6583 - val_acc: 0.6510\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 154us/step - loss: 1.4356 - acc: 0.7058 - val_loss: 1.4051 - val_acc: 0.6890\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 147us/step - loss: 1.1904 - acc: 0.7388 - val_loss: 1.2579 - val_acc: 0.7160\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 151us/step - loss: 1.0195 - acc: 0.7730 - val_loss: 1.1668 - val_acc: 0.7350\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 150us/step - loss: 0.8803 - acc: 0.8086 - val_loss: 1.0953 - val_acc: 0.7650\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 156us/step - loss: 0.7633 - acc: 0.8351 - val_loss: 1.0453 - val_acc: 0.7710\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 152us/step - loss: 0.6606 - acc: 0.8558 - val_loss: 1.0001 - val_acc: 0.7920\n",
      "2246/2246 [==============================] - 0s 155us/step\n",
      "[1.0652385821643942, 0.76402493326768]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 282us/step - loss: 2.2061 - acc: 0.5724 - val_loss: 1.3878 - val_acc: 0.6750\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 2s 218us/step - loss: 1.1045 - acc: 0.7628 - val_loss: 1.1935 - val_acc: 0.7390\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 2s 241us/step - loss: 0.7730 - acc: 0.8327 - val_loss: 0.9914 - val_acc: 0.7770\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 2s 249us/step - loss: 0.5709 - acc: 0.8775 - val_loss: 0.8829 - val_acc: 0.8200\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 2s 228us/step - loss: 0.4130 - acc: 0.9133 - val_loss: 0.8536 - val_acc: 0.8240\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 2s 233us/step - loss: 0.3195 - acc: 0.9326 - val_loss: 0.8566 - val_acc: 0.8220\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 2s 236us/step - loss: 0.2579 - acc: 0.9414 - val_loss: 0.9058 - val_acc: 0.8080\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 2s 227us/step - loss: 0.1988 - acc: 0.9510 - val_loss: 0.9351 - val_acc: 0.8150\n",
      "2246/2246 [==============================] - 1s 261us/step\n",
      "[1.040991757877788, 0.7871772039446148]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 2.3660 - acc: 0.4746 - val_loss: 1.5676 - val_acc: 0.6370\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 2s 225us/step - loss: 1.2717 - acc: 0.7105 - val_loss: 1.2265 - val_acc: 0.7200\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 2s 224us/step - loss: 0.9407 - acc: 0.7783 - val_loss: 1.2727 - val_acc: 0.7220\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 2s 227us/step - loss: 0.7062 - acc: 0.8319 - val_loss: 1.0743 - val_acc: 0.7610\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 2s 233us/step - loss: 0.5138 - acc: 0.8809 - val_loss: 1.1114 - val_acc: 0.7800\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 2s 227us/step - loss: 0.4264 - acc: 0.8996 - val_loss: 0.9799 - val_acc: 0.8020\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 2s 229us/step - loss: 0.2943 - acc: 0.9307 - val_loss: 1.0570 - val_acc: 0.7990\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 2s 236us/step - loss: 0.2821 - acc: 0.9334 - val_loss: 1.0396 - val_acc: 0.8020\n",
      "2246/2246 [==============================] - 1s 254us/step\n",
      "[1.1244304231098476, 0.7853962600708857]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 228us/step - loss: 2.7342 - acc: 0.4618 - val_loss: 1.6820 - val_acc: 0.6330\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 173us/step - loss: 1.4600 - acc: 0.6693 - val_loss: 1.3765 - val_acc: 0.6790\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 175us/step - loss: 1.1551 - acc: 0.7357 - val_loss: 1.1998 - val_acc: 0.7340\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 174us/step - loss: 0.9319 - acc: 0.7913 - val_loss: 1.1552 - val_acc: 0.7400\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 175us/step - loss: 0.7422 - acc: 0.8398 - val_loss: 1.0962 - val_acc: 0.7660\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 2s 189us/step - loss: 0.6122 - acc: 0.8641 - val_loss: 1.1691 - val_acc: 0.7550\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 174us/step - loss: 0.5026 - acc: 0.8884 - val_loss: 1.0934 - val_acc: 0.7850\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 177us/step - loss: 0.4066 - acc: 0.9144 - val_loss: 1.1361 - val_acc: 0.7660\n",
      "2246/2246 [==============================] - 0s 193us/step\n",
      "[1.209887535149565, 0.7448797862336557]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 3.2212 - acc: 0.3380 - val_loss: 2.3589 - val_acc: 0.5550\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 150us/step - loss: 1.8800 - acc: 0.6361 - val_loss: 1.6272 - val_acc: 0.6520\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 1.4070 - acc: 0.7017 - val_loss: 1.4768 - val_acc: 0.6600\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 1.1977 - acc: 0.7293 - val_loss: 1.3193 - val_acc: 0.7030\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 153us/step - loss: 1.0544 - acc: 0.7494 - val_loss: 1.2298 - val_acc: 0.7140\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 155us/step - loss: 0.9204 - acc: 0.7798 - val_loss: 1.1979 - val_acc: 0.7280\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.8177 - acc: 0.8071 - val_loss: 1.1425 - val_acc: 0.7370\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 156us/step - loss: 0.7313 - acc: 0.8241 - val_loss: 1.1197 - val_acc: 0.7490\n",
      "2246/2246 [==============================] - 0s 172us/step\n",
      "[1.2117631189120421, 0.7359750668384725]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
